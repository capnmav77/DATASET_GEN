{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eNuhDhnIyVrD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (4.66.2)\n",
            "Requirement already satisfied: openai==0.28 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/llmao/miniconda3/envs/cosumer-venv/lib/python3.10/site-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q openai\n",
        "# !pip install -q langchain\n",
        "# !pip install -q guardrails-ai\n",
        "# !pip install -q faiss-cpu\n",
        "# !pip install -q pypdf\n",
        "# !pip install -q python-dotenv\n",
        "# !pip install -q datasets\n",
        "# !pip install -q huggingface_hub\n",
        "# !pip install tqdm\n",
        "# !pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l7p-KPRTyVrG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage,  SystemMessage\n",
        "import openai\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcCaQB8yVrG"
      },
      "source": [
        "## Name of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CC93-S1WyVrH"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"YOUR_DATASET_NAME\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7lmTPBZyVrI"
      },
      "source": [
        "## Load Environment Varible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "openai.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"OpenAI API key set successfully:\", openai.api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Los Angeles Dodgers won the 2020 World Series.\n"
          ]
        }
      ],
      "source": [
        "# response = openai.ChatCompletion.create(\n",
        "#  model=\"gpt-3.5-turbo\",\n",
        "#  messages=[\n",
        "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "#  ]\n",
        "# )\n",
        "\n",
        "# print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA ANALYTICS AND CHUNKING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_file_generator(file_path):\n",
        "    \"\"\"\n",
        "    Generator function to efficiently read a file line by line.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            yield line.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data_into_chunks(data, chunk_size=2000, chunk_overlap=500):\n",
        "    \"\"\"\n",
        "    Split the extracted data into fixed-size text chunks.\n",
        "\n",
        "    Parameters:\n",
        "        - data (list): List of dictionaries containing \"page_content\" and \"metadata\".\n",
        "        - chunk_size (int): Size of each text chunk.\n",
        "        - chunk_overlap (int): Overlap between adjacent chunks.\n",
        "\n",
        "    Returns:\n",
        "        - list: List of text chunks.\n",
        "    \"\"\"\n",
        "    text_chunks = []\n",
        "    \n",
        "    for entry in data:\n",
        "        content = entry.get(\"page_content\", \"\")\n",
        "        metadata = entry.get(\"metadata\", {})\n",
        "        \n",
        "        for start in range(0, len(content), chunk_size - chunk_overlap):\n",
        "            end = start + chunk_size\n",
        "            text_chunk = content[start:end]\n",
        "            \n",
        "            text_chunks.append({\n",
        "                \"page_content\": text_chunk,\n",
        "                \"metadata\": metadata\n",
        "            })\n",
        "\n",
        "    return text_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA PROCESSING AND GENERATION OF DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_to_jsonl(dataset_name, question_answer_list):\n",
        "    \"\"\"\n",
        "    Save question-answer pairs with metadata to a JSONL file.\n",
        "\n",
        "    Parameters:\n",
        "        - dataset_name (str): The name of the dataset.\n",
        "        - question_answer_list (list): List of dictionaries containing question, answer, and metadata.\n",
        "    \"\"\"\n",
        "    file_name = f\"/home/llmao/fastapi/DataGen/output/{dataset_name}.jsonl\"\n",
        "\n",
        "    with open(file_name, \"a\", encoding=\"utf-8\") as file:\n",
        "        for qa in question_answer_list:\n",
        "            # Combine question, answer, and metadata into a dictionary\n",
        "            entry = {\n",
        "                \"question\": qa[\"question\"],\n",
        "                \"answer\": qa[\"answer\"],\n",
        "                \"content\": qa.get(\"content\", \"\"),\n",
        "                \"text\": f\"[INST] <<SYS>> You are Shambu, a helpful, respectful, honest, and a Personal Healthcare assistant. Always answer as helpfully as possible, while being safe. If you don’t know the answer to a question, please don’t share false information. <</SYS>> {qa['question']} [/INST] {qa['answer']}\"\n",
        "            }\n",
        "            # Write the dictionary to the JSONL file\n",
        "            file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def process_text_chunk(text,dataset_name):\n",
        "    json_response_format = [\n",
        "        {\n",
        "            \"question\": \"In the context of ...\",\n",
        "            \"answer\": \"...\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"In the context of ...\",\n",
        "            \"answer\": \"...\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"In the context of ...\",\n",
        "            \"answer\": \"...\"\n",
        "        }\n",
        "    ]\n",
        "    # Short Response    \n",
        "    short_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are an expert at qurating/generating questions and answers from a given piece of text.\n",
        "                            The questions and answers you generate are unique from one another and are not repeated.\n",
        "                            You always respond in the following json format ```question_answer:{json_response_format}```\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                # you should add the summary and modify the prompt to your liking\n",
        "                \"content\": f\"\"\"given the context which is about *summary of the document* \n",
        "                \\n\n",
        "                {text['page_content']} \n",
        "                \\n\n",
        "                provide 5 important questions and answers pairs base on the text above , \n",
        "                The Question must begin with \"In the context of ...\\\".The answer borrow, verbatim, from the text above. \n",
        "                In providing each question consider that the reader does not see or have access to any of the other questions from context. \n",
        "                Vary the style and format of questions. Let the answers be descriptive around 100 to 200 words\n",
        "                \"\"\"\n",
        "                # Respond in only JSON following this format and nothing else {json_response_format}\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        short_response_output = eval(short_response.choices[0].message.content)\n",
        "        # Check if the format is correct and meets the criteria\n",
        "        if (\n",
        "            isinstance(short_response_output, dict)\n",
        "            and \"question_answer\" in short_response_output\n",
        "            and isinstance(short_response_output[\"question_answer\"], list)\n",
        "            and len(short_response_output[\"question_answer\"]) >= 3\n",
        "            and all(\n",
        "                isinstance(qa, dict)\n",
        "                and \"question\" in qa\n",
        "                and \"answer\" in qa\n",
        "                for qa in short_response_output[\"question_answer\"]\n",
        "            )\n",
        "        ):\n",
        "            # Add metadata to each question-answer pair\n",
        "            for qa in short_response_output[\"question_answer\"]:\n",
        "                qa[\"content\"] = f\"{text['page_content']}\"\n",
        "                qa[\"metadata\"] = text['metadata']\n",
        "\n",
        "            # print(\"Short response format is correct.\")\n",
        "        else:\n",
        "            print(\"Short response format is incorrect. Running the query again.\")\n",
        "            \n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error in short_response_output\", e)\n",
        "\n",
        "    # Long Response\n",
        "    long_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.3,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are an expert at qurating/generating questions and answers from a given piece of text.\n",
        "                            The questions and answers you generate are unique from one another and are not repeated.\n",
        "                            You always respond in the following json format ```question_answer:{json_response_format}```\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"given the context which is about Tao Science which is written by \"Rulin Xiu\" and \"Zhi Gang Sha\"\n",
        "                \\n\n",
        "                {text['page_content']} \n",
        "                \\n\n",
        "                provide 4 important questions and answers pairs base on the text above , \n",
        "                The Question must begin with \"In the context of...\\\".The answer borrow, verbatim, from the text above. \n",
        "                In providing each question consider that the reader does not see or have access to any of the other questions from context. \n",
        "                Vary the style and format of questions. Let the answers be descriptive and lengthy.\n",
        "                The answer should at least be 1000 words\n",
        "                \"\"\"\n",
        "                # Respond in only JSON following this format and nothing else {json_response_format}\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        long_response_output = eval(long_response.choices[0].message.content)\n",
        "\n",
        "        # Check if the format is correct and meets the criteria\n",
        "        if (\n",
        "            isinstance(long_response_output, dict)\n",
        "            and \"question_answer\" in long_response_output\n",
        "            and isinstance(long_response_output[\"question_answer\"], list)\n",
        "            and len(long_response_output[\"question_answer\"]) >= 2\n",
        "            and all(\n",
        "                isinstance(qa, dict)\n",
        "                and \"question\" in qa\n",
        "                and \"answer\" in qa\n",
        "                for qa in long_response_output[\"question_answer\"]\n",
        "            )\n",
        "        ):\n",
        "            # Add metadata to each question-answer pair\n",
        "            for qa in long_response_output[\"question_answer\"]:\n",
        "                qa[\"content\"] = f\"{text['page_content']}\"\n",
        "                qa[\"metadata\"] = text['metadata']\n",
        "\n",
        "            # print(\"Long response format is correct.\")\n",
        "        else:\n",
        "            print(\"Long response format is incorrect. Running the query again.\")\n",
        "            \n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error in long_response_output\", e)\n",
        "    save_to_jsonl(dataset_name, short_response_output[\"question_answer\"])\n",
        "    save_to_jsonl(dataset_name, long_response_output[\"question_answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HUGGINGFACE HUG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def huggingface_upload(dataset_name):\n",
        "    hf_token = \"YOUT_API_KEY_HERE\"\n",
        "    subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_token])\n",
        "    dataset = load_dataset('json', data_files=f\"/home/llmao/fastapi/DataGen/output/{dataset_name}.jsonl\")\n",
        "    dataset\n",
        "    dataset.push_to_hub(f\"LLMao/{dataset_name}\")\n",
        "    print(\"successfully pushed to HF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MAIN FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def process_and_store_data(dataset_name):\n",
        "    # Set the OpenAI API key\n",
        "    openai.api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    \"\"\"\n",
        "    Read content from files in a folder, ignore empty documents, and generate data.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    for filename in os.listdir(\"/home/llmao/fastapi/DataGen/corpus\"):\n",
        "        file_path = os.path.join(\"/home/llmao/fastapi/DataGen/corpus\", filename)\n",
        "        if os.path.isfile(file_path) and os.path.getsize(file_path) > 0:\n",
        "            # Ignore empty documents\n",
        "            document = {\n",
        "                \"page_content\": \"\", \n",
        "                \"metadata\": {\"source\": file_path, \"page\": 0},\n",
        "            }\n",
        "\n",
        "            # Read content from the file\n",
        "            document[\"page_content\"] = \"\\n\".join(read_file_generator(file_path))\n",
        "\n",
        "            data.append(document)\n",
        "\n",
        "    # Save the resulting data to a file\n",
        "    # output_file_path = f\"/home/llmao/fastapi/DataGen/output/{dataset_name}.jsonl\"\n",
        "    # with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    #     for entry in data:\n",
        "    #         output_file.write(json.dumps(entry) + \"\\n\")\n",
        "    \n",
        "    print(data[:1])\n",
        "\n",
        "    if(data == \"\"):\n",
        "        print(\"error parsing the files , or no files parsed . \\n exiting function \")\n",
        "        return \n",
        "\n",
        "    chunk_size = 2500\n",
        "    chunk_overlap = 500\n",
        "    text_chunks = split_data_into_chunks(data, chunk_size, chunk_overlap)\n",
        "    print(\"Length of the whole documentation is:\", len(text_chunks),end=\"\\n\")\n",
        "\n",
        "    print(text_chunks[1]['page_content'],end=\"\\n\")\n",
        "\n",
        "    # Set the number of parallel processes\n",
        "    num_processes = 4  #optimal performance without crashing !!\n",
        "\n",
        "    # # Use a ThreadPoolExecutor for parallel execution\n",
        "    # with concurrent.futures.ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
        "    #     # Define the function to be executed in parallel\n",
        "    #     future_to_chunk = {executor.submit(process_text_chunk, text_chunk, dataset_name): text_chunk for text_chunk in text_chunks}\n",
        "\n",
        "    #     # Use tqdm to track progress\n",
        "    #     for future in tqdm(concurrent.futures.as_completed(future_to_chunk), total=len(text_chunks), desc=\"Processing Text Chunks\"):\n",
        "    #         pass  # Processing happens in parallel, tqdm just tracks completion\n",
        "    print(\"initializing the model to generate structured data\")\n",
        "\n",
        "    for text_chunk in text_chunks:\n",
        "        process_text_chunk(text_chunk,dataset_name)\n",
        "        \n",
        "    print(f\"structured_data generated , stored in {dataset_name} , uploading to huggingface\")\n",
        "    huggingface_upload(dataset_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
